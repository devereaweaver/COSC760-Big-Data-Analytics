





# Download files from HDFS into working environment
! [ -d monte-carlo-risk ] || (echo "Downloading prepared data from HDFS. Please wait..." ; \
                    hdfs dfs -copyToLocal /financial_risk . ; echo "Done!";)


# Read data into a PySpark DataFrame using SparkSql
from pyspark.sql import SparkSession

# Step 1: Create a SparkSession
spark = SparkSession.builder \
    .appName("Read CSV from HDFS") \
    .getOrCreate()

# Step 2: Define the HDFS folder path where your CSV files are stored
#hdfs_folder_path = "hdfs://namenode:port/path/to/csv_folder/"
crude_oil_path = "financial_risk/crudeoil.csv"
tbond_path = "financial_risk/crudeoil.csv"

# Step 3: Read CSV files into a PySpark DataFrame
# This will read all CSV files in the specified folder
crude_oil_df = spark.read.csv(crude_oil_path, header=True, inferSchema=True)
tbond_df = spark.read.csv(tbond_path, header=True, inferSchema=True)

# Step 4: Show the DataFrame (optional)
crude_oil_df.show()
tbond_df.show()

# Step 5: Stop the Spark session (optional if script ends here)
spark.stop()



# Plot the crude oil price over time
from pyspark.sql import SparkSession
import pandas as pd
import matplotlib.pyplot as plt

# Initialize Spark session
spark = SparkSession.builder.appName("Plot Crude Oil Prices").getOrCreate()

# Load the crudeoil.csv file into a PySpark DataFrame
crude_oil_df = spark.read.csv('financial_risk/crudeoil.csv', header=True, inferSchema=True)

# Display the schema to check column names and types
crude_oil_df.printSchema()

# Convert the PySpark DataFrame to a Pandas DataFrame
crudeoil_pd = crude_oil_df.toPandas()

# Ensure the Date column is in datetime format
crudeoil_pd['Date'] = pd.to_datetime(crudeoil_pd['Date'])

# Sort the data by Date for proper plotting
crudeoil_pd = crudeoil_pd.sort_values('Date')

# Plot the data
plt.figure(figsize=(12, 6))
plt.plot(crudeoil_pd['Date'], crudeoil_pd['Price'], label='Crude Oil Price', color='blue', marker='o')

# Adding title, labels, and legend
plt.title('Crude Oil Price Over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.show();



# Plot T-bonds over time
from pyspark.sql import SparkSession
import pandas as pd
import matplotlib.pyplot as plt

# Initialize Spark session
spark = SparkSession.builder.appName("Plot Treasury Bond Prices").getOrCreate()

# Load the us30yeartreasurybonds.csv file into a PySpark DataFrame
treasurybonds_df = spark.read.csv('financial_risk/us30yeartreasurybonds.csv', header=True, inferSchema=True)

# Display the schema to check column names and types
treasurybonds_df.printSchema()

# Convert the PySpark DataFrame to a Pandas DataFrame
treasurybonds_pd = treasurybonds_df.toPandas()

# Ensure the Date column is in datetime format
treasurybonds_pd['Date'] = pd.to_datetime(treasurybonds_pd['Date'])

# Sort the data by Date for proper plotting
treasurybonds_pd = treasurybonds_pd.sort_values('Date')

# Plot the data
plt.figure(figsize=(12, 6))
plt.plot(treasurybonds_pd['Date'], treasurybonds_pd['Price'], label='30-Year Treasury Bond Price', color='green', marker='x')

# Adding title, labels, and legend
plt.title('30-Year Treasury Bond Price Over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.show();



# Install yfinance if not already installed
!pip install yfinance


# Import teh GSPC and IXIC data from January 1950 to January 2014
import yfinance as yf

# Define the ticker symbol and date range
ticker = "^GSPC"
start_date = "1950-01-01"
end_date = "2014-01-01"

# Fetch the data using yfinance
gspc_data = yf.download(ticker, start=start_date, end=end_date)

# Display the first few rows of the data
gspc_data.head()

# Save the data to a CSV file (optional)
#gspc_data.to_csv("GSPC_1950_2014.csv")

# Quick summary of the data
#print(gspc_data.info())


# Plot GSPC over time 
# Plot the 'Close' attribute over time
plt.figure(figsize=(12, 6))
plt.plot(gspc_data.index, gspc_data['Close'], label='S&P 500 Close Price', color='blue')

# Adding title, labels, and legend
plt.title('S&P 500 Close Price (1950 - 2014)')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show();


# Download IXIC data 

# Define the ticker symbol and date range
ticker = "^IXIC"
start_date = "1950-01-01"
end_date = "2014-01-01"

# Fetch the data using yfinance
ixic_data = yf.download(ticker, start=start_date, end=end_date)

# Display the first few rows of the data
ixic_data.head()

# Save the data to a CSV file (optional)
#ixic_data.to_csv("IXIC_1950_2014.csv")

# Quick summary of the data
#print(ixic_data.info())



# Plot IXIC data 
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf

# Plot the 'Close' attribute over time
plt.figure(figsize=(12, 6))
plt.plot(ixic_data.index, ixic_data['Close'], label='NASDAQ Composite Close Price', color='green')

# Adding title, labels, and legend
plt.title('NASDAQ Composite Close Price (1971 - 2014)')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show();



# Convert the Pandas DataFrames into PySPark DataFrames
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("Data Conversion with Date").getOrCreate()

# Reset index to include the Date column in the Pandas DataFrames
ixic_data_reset = ixic_data.reset_index()
gspc_data_reset = gspc_data.reset_index()

# Convert Pandas DataFrames to PySpark DataFrames
ixic_spark_df = spark.createDataFrame(ixic_data_reset)
gspc_spark_df = spark.createDataFrame(gspc_data_reset)

# Display schema of the PySpark DataFrames to confirm the Date column is present
ixic_spark_df.printSchema()
gspc_spark_df.printSchema()

# Show a few rows from each PySpark DataFrame
ixic_spark_df.show(5)
gspc_spark_df.show(5)







# Fetch code for 35 random well-known stock stickers
import yfinance as yf
import pandas as pd

# List of 35 large, well-known stock tickers (you can customize this list)
tickers = [
    "AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "META", "NVDA", "BRK-B", "JPM", "JNJ",
    "V", "PG", "XOM", "UNH", "HD", "MA", "BAC", "PFE", "KO", "PEP",
    "ABBV", "T", "COST", "WMT", "CSCO", "DIS", "CVX", "ADBE", "NFLX", "MRK",
    "ORCL", "NKE", "CRM", "INTC", "MCD"
]

# Date range for historical data
start_date = "2000-01-01"
end_date = "2023-12-31"

# Dictionary to store each stock's data as a Pandas DataFrame
stock_dataframes = {}

# Loop through each ticker, fetch data, and store in the dictionary
for ticker in tickers:
    print(f"Downloading data for {ticker}...")
    stock_data = yf.download(ticker, start=start_date, end=end_date)
    stock_dataframes[ticker] = stock_data
    print(f"Data for {ticker} downloaded. Total records: {len(stock_data)}")

# Save each DataFrame to a unique CSV file (optional)
for ticker, data in stock_dataframes.items():
    filename = f"{ticker}_data.csv"
    data.to_csv(filename)
    print(f"Saved {ticker} data to {filename}")

# Example: Access AAPL's DataFrame
print(stock_dataframes["AAPL"].head())



# Verify download
# Loop through the dictionary and display the first two rows for each stock
for ticker, dataframe in stock_dataframes.items():
    print(f"First two rows for {ticker}:")
    print(dataframe.head(2))  # Display the first 2 rows
    print("-" * 50)  # Separator for better readability



from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("Convert Stocks to PySpark").getOrCreate()

# Convert each Pandas DataFrame to a PySpark DataFrame and create standalone objects
for ticker, dataframe in stock_dataframes.items():
    # Reset the index to include the Date column
    dataframe_reset = dataframe.reset_index()
    # Dynamically create a PySpark DataFrame variable for each ticker
    globals()[f"{ticker}_spark_df"] = spark.createDataFrame(dataframe_reset)
    print(f"Created PySpark DataFrame for {ticker}.")



AAPL_spark_df.show(5)


MSFT_spark_df.show(5)



