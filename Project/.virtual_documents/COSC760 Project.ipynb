





# Download files from HDFS into working environment
! [ -d monte-carlo-risk ] || (echo "Downloading prepared data from HDFS. Please wait..." ; \
                    hdfs dfs -copyToLocal /financial_risk . ; echo "Done!";)


# Read data into a PySpark DataFrame using SparkSql
from pyspark.sql import SparkSession

# Step 1: Create a SparkSession
spark = SparkSession.builder \
    .appName("Read CSV from HDFS") \
    .getOrCreate()

# Step 2: Define the HDFS folder path where your CSV files are stored
#hdfs_folder_path = "hdfs://namenode:port/path/to/csv_folder/"
crude_oil_path = "financial_risk/crudeoil.csv"
tbond_path = "financial_risk/crudeoil.csv"

# Step 3: Read CSV files into a PySpark DataFrame
# This will read all CSV files in the specified folder
crude_oil_df = spark.read.csv(crude_oil_path, header=True, inferSchema=True)
tbond_df = spark.read.csv(tbond_path, header=True, inferSchema=True)

# Step 4: Show the DataFrame (optional)
crude_oil_df.show(5)
tbond_df.show(5)

# Step 5: Stop the Spark session (optional if script ends here)
#spark.stop()



# Plot the crude oil price over time
from pyspark.sql import SparkSession
import pandas as pd
import matplotlib.pyplot as plt

# Initialize Spark session
spark = SparkSession.builder.appName("Plot Crude Oil Prices").getOrCreate()

# Load the crudeoil.csv file into a PySpark DataFrame
crude_oil_df = spark.read.csv('financial_risk/crudeoil.csv', header=True, inferSchema=True)

# Display the schema to check column names and types
crude_oil_df.printSchema()

# Convert the PySpark DataFrame to a Pandas DataFrame
crudeoil_pd = crude_oil_df.toPandas()

# Ensure the Date column is in datetime format
crudeoil_pd['Date'] = pd.to_datetime(crudeoil_pd['Date'])

# Sort the data by Date for proper plotting
crudeoil_pd = crudeoil_pd.sort_values('Date')

# Plot the data
plt.figure(figsize=(12, 6))
plt.plot(crudeoil_pd['Date'], crudeoil_pd['Price'], label='Crude Oil Price', color='blue', marker='o')

# Adding title, labels, and legend
plt.title('Crude Oil Price Over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.show();



# Plot T-bonds over time
from pyspark.sql import SparkSession
import pandas as pd
import matplotlib.pyplot as plt

# Initialize Spark session
spark = SparkSession.builder.appName("Plot Treasury Bond Prices").getOrCreate()

# Load the us30yeartreasurybonds.csv file into a PySpark DataFrame
treasurybonds_df = spark.read.csv('financial_risk/us30yeartreasurybonds.csv', header=True, inferSchema=True)

# Display the schema to check column names and types
treasurybonds_df.printSchema()

# Convert the PySpark DataFrame to a Pandas DataFrame
treasurybonds_pd = treasurybonds_df.toPandas()

# Ensure the Date column is in datetime format
treasurybonds_pd['Date'] = pd.to_datetime(treasurybonds_pd['Date'])

# Sort the data by Date for proper plotting
treasurybonds_pd = treasurybonds_pd.sort_values('Date')

# Plot the data
plt.figure(figsize=(12, 6))
plt.plot(treasurybonds_pd['Date'], treasurybonds_pd['Price'], label='30-Year Treasury Bond Price', color='green', marker='x')

# Adding title, labels, and legend
plt.title('30-Year Treasury Bond Price Over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.show();



# Install yfinance if not already installed
!pip install yfinance


# Import teh GSPC and IXIC data from January 1950 to January 2014
import yfinance as yf
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("Convert GSPC to PySpark").getOrCreate()

# Define the ticker symbol and date range
ticker = "^GSPC"
start_date = "1950-01-01"
end_date = "2014-01-01"

# Download GSPC data
print(f"Downloading data for {ticker}...")
gspc_data = yf.download(ticker, start=start_date, end=end_date)
gspc_data.reset_index(inplace=True)  # Ensure 'Date' is a column, not an index

# Standardize column names
gspc_data.columns = [
    "Date",  # Index reset ensures this is the first column
    "Open", "High", "Low", "Close", "Adj Close", "Volume"
]

# Reorder columns to match the desired order
gspc_data = gspc_data[["Date", "Adj Close", "Close", "High", "Low", "Open", "Volume"]]

# Convert to PySpark DataFrame
GSPC_spark_df = spark.createDataFrame(gspc_data)
print(f"Converted {ticker} data to PySpark DataFrame.")

# Show schema and data for verification
GSPC_spark_df.printSchema()
GSPC_spark_df.show(5)



# Plot GSPC over time 
# Plot the 'Close' attribute over time
plt.figure(figsize=(12, 6))
plt.plot(gspc_data.index, gspc_data['Close'], label='S&P 500 Close Price', color='blue')

# Adding title, labels, and legend
plt.title('S&P 500 Close Price (1950 - 2014)')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show();


# Download IXIC data 

import yfinance as yf
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("Convert IXIC to PySpark").getOrCreate()

# Define the ticker symbol and date range
ticker = "^IXIC"
start_date = "1971-01-01"
end_date = "2014-01-01"

# Download IXIC data
print(f"Downloading data for {ticker}...")
ixic_data = yf.download(ticker, start=start_date, end=end_date)
ixic_data.reset_index(inplace=True)  # Ensure 'Date' is a column, not an index

# Standardize column names
ixic_data.columns = [
    "Date",  # Index reset ensures this is the first column
    "Open", "High", "Low", "Close", "Adj Close", "Volume"
]

# Reorder columns to match the desired order
ixic_data = ixic_data[["Date", "Adj Close", "Close", "High", "Low", "Open", "Volume"]]

# Convert to PySpark DataFrame
IXIC_spark_df = spark.createDataFrame(ixic_data)
print(f"Converted {ticker} data to PySpark DataFrame.")

# Show schema and data for verification
IXIC_spark_df.printSchema()
IXIC_spark_df.show(5)



# Plot IXIC data 
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf

# Plot the 'Close' attribute over time
plt.figure(figsize=(12, 6))
plt.plot(ixic_data.index, ixic_data['Close'], label='NASDAQ Composite Close Price', color='green')

# Adding title, labels, and legend
plt.title('NASDAQ Composite Close Price (1971 - 2014)')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show();



# Convert the Pandas DataFrames into PySPark DataFrames
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("Data Conversion with Date").getOrCreate()

# Reset index to include the Date column in the Pandas DataFrames
ixic_data_reset = ixic_data.reset_index()
gspc_data_reset = gspc_data.reset_index()

# Convert Pandas DataFrames to PySpark DataFrames
ixic_spark_df = spark.createDataFrame(ixic_data_reset)
gspc_spark_df = spark.createDataFrame(gspc_data_reset)

# Display schema of the PySpark DataFrames to confirm the Date column is present
ixic_spark_df.printSchema()
gspc_spark_df.printSchema()

# Show a few rows from each PySpark DataFrame
ixic_spark_df.show(5)
gspc_spark_df.show(5)










# Import 25 different stocks and standardize their headers aLowLow
import yfinance as yf
import pandas as pd

# List of 25 large, well-known stock tickers
tickers = [
    "AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "META", "NVDA", "VZ", "JPM", "JNJ",
    "V", "PG", "XOM", "UNH", "HD", "MA", "BAC", "PFE", "KO", "PEP",
    "ABBV", "T", "COST", "WMT", "CSCO"
]

# Date range for historical data
start_date = "2000-01-01"
end_date = "2023-12-31"

# Desired column headers
standard_columns = ["Date", "Adj Close", "Close", "High", "Low", "Open", "Volume"]

# Loop through each ticker, fetch data, and create standalone variables
for ticker in tickers:
    print(f"Downloading data for {ticker}...")
    stock_data = yf.download(ticker, start=start_date, end=end_date)
    stock_data.reset_index(inplace=True)  # Ensure 'Date' is a column, not an index

    # Rename columns to match the desired format
    stock_data.columns = [
        "Date",  # Index reset ensures this is the first column
        "Open", "High", "Low", "Close", "Adj Close", "Volume"
    ]

    # Reorder columns to match the desired order
    stock_data = stock_data[standard_columns]

    # Dynamically create a standalone variable for the DataFrame
    globals()[f"{ticker}_df"] = stock_data
    print(f"Created DataFrame for {ticker} as standalone variable.")





# Convert them to PySpark DataFrames
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("Convert Stocks to PySpark").getOrCreate()

# List of tickers
tickers = [
    "AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "META", "NVDA", "VZ", "JPM", "JNJ",
    "V", "PG", "XOM", "UNH", "HD", "MA", "BAC", "PFE", "KO", "PEP",
    "ABBV", "T", "COST", "WMT", "CSCO"
]

# Convert each Pandas DataFrame to a PySpark DataFrame
for ticker in tickers:
    # Access the Pandas DataFrame dynamically using globals()
    pandas_df = globals()[f"{ticker}_df"]
    
    # Convert to PySpark DataFrame
    pyspark_df = spark.createDataFrame(pandas_df)
    
    # Dynamically create a standalone PySpark DataFrame variable
    globals()[f"{ticker}_spark_df"] = pyspark_df
    print(f"Converted {ticker}_df to {ticker}_spark_df.")



# Test
TSLA_spark_df.printSchema()


MSFT_spark_df.show(5)





# Try to change date format for all PySpark DataFrames
from pyspark.sql.functions import to_date, col

# List of your PySpark DataFrames
pyspark_dfs = [
    GSPC_spark_df, IXIC_spark_df, AAPL_spark_df, MSFT_spark_df, AMZN_spark_df, 
    TSLA_spark_df, META_spark_df, NVDA_spark_df, VZ_spark_df, JPM_spark_df,
    JNJ_spark_df, V_spark_df, PG_spark_df, XOM_spark_df, UNH_spark_df,
    HD_spark_df, MA_spark_df, BAC_spark_df, PFE_spark_df, KO_spark_df,
    PEP_spark_df, ABBV_spark_df, T_spark_df, COST_spark_df, WMT_spark_df,
    CSCO_spark_df
]

# Function to standardize the Date column for a single DataFrame
def standardize_date_format(df):
    return df.withColumn("Date", to_date(col("Date"), "yyyy-MM-dd"))

# Loop through the list of DataFrames and apply the date standardization
for i, df in enumerate(pyspark_dfs):
    pyspark_dfs[i] = standardize_date_format(df)
    print(f"Standardized Date format for DataFrame {i + 1}")



# Count the number of rows in the DataFrame
num_observations = tbond_df.count()

# Print the result
print(f"The DataFrame has {num_observations} observations.")






AAPL_spark_df.show(5)


from pyspark.sql.window import Window
from pyspark.sql.functions import col, lag#, ((col("price") - lag("price").over(window_spec)) / lag("price").over(window_spec))
from pyspark.sql.functions import lit


# Define a window partitioned by the stock (if needed) and ordered by date
window_spec = Window.orderBy("Date")

# Calculate the percentage return
returns_df_1 = AAPL_spark_df.withColumn(
    "return", 
    (col("Close") - lag("Close").over(window_spec)) / lag("Close").over(window_spec)
)

returns_df_2 = MSFT_spark_df.withColumn(
    "return", 
    (col("Close") - lag("Close").over(window_spec)) / lag("Close").over(window_spec)
)

# Filter out rows with null returns (first row will have null return since there's no previous price)
returns_df_1 = returns_df_1.filter(col("return").isNotNull())
returns_df_2 = returns_df_2.filter(col("return").isNotNull())

# Add an instrument column to identify the stock
returns_df_1 = returns_df_1.withColumn("instrument", lit("AAPL"))
returns_df_2 = returns_df_2.withColumn("instrument", lit("MSFT"))

# Show the result
returns_df_1.show(1)
returns_df_2.show(1)



# Combine all returns DataFrames
portfolio_df = returns_df_1.union(returns_df_2)


portfolio_df.show()





# Aggregate using assigned weights
from pyspark.sql.functions import when, col

# Define portfolio weights (example weights)
weights = {"AAPL": 0.4, "MSFT": 0.6}

# Add a weight column based on the instrument
portfolio_df = portfolio_df.withColumn(
    "weight",
    when(col("instrument") == "AAPL", weights["AAPL"])
    .when(col("instrument") == "MSFT", weights["MSFT"])
    .otherwise(0.0)  # Default weight if instrument not found
)

# Show the result
portfolio_df.show()



# Calculate the weighted return
portfolio_df = portfolio_df.withColumn("weighted_return", col("return") * col("weight"))


portfolio_df.show(5)



